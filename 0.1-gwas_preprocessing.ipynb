{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# often used imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from pickle import dump\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from operator import itemgetter\n",
    "from sklearn.impute import KNNImputer\n",
    "import json\n",
    "\n",
    "# turn off warnings and set up logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
    "\n",
    "# environment variables\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "dotenv_path = find_dotenv()\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set(rc={'figure.figsize': (16, 9.)})\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "pd.set_option('display.max_rows', 120)\n",
    "pd.set_option('display.max_columns', 120)\n",
    "\n",
    "# automatically reload packages when they are edited\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from blood_response.gwas_preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configure paths and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = 'sysmex_custom_gates_v9'\n",
    "gwas_run = '2020_10_28'\n",
    "gwas_path = f'/mnt/obi0/phi/gwas/obi_gwas/runs/{gwas_run}/imputed_plus_biobank/'\n",
    "analysis_path = f'/mnt/obi0/phi/gwas/gwas_analyses/{analysis_name}-obi{gwas_run}/'\n",
    "patient_ids = pd.read_csv(f'{gwas_path}/patient_ids_all.tsv', sep='\\t', dtype=str)\n",
    "covariates_df = pd.read_parquet('/mnt/obi0/phi/ehr/obi_biobank_annotations/covariates_21-07-06.parquet')\n",
    "\n",
    "perturbation_names = pd.read_excel(analysis_path + '/perturbation.time.dose.ID.summary.xlsx')\n",
    "perturbation_names['id_str'] = perturbation_names.id_number.apply(lambda x: f'{x:03d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redcap with blood draw times\n",
    "redcap_blood_draws = pd.read_parquet('/mnt/obi0/phi/sysmex/redcap/2021_07_06/redcap_phlebotomy_validated.parquet')\n",
    "redcap_blood_draws.SpecimenTakenTimeDTS.dt.hour.hist(bins=9)\n",
    "\n",
    "# medications, transplants etc\n",
    "anticoagulants_30d = \\\n",
    "pd.read_parquet('/mnt/obi0/mhomilius/projects/sysmex/data/obi_biobank_annotations/anticoagulant_obi_30d_21-07-06.parquet')\n",
    "steroids_30d = \\\n",
    "pd.read_parquet('/mnt/obi0/mhomilius/projects/sysmex/data/obi_biobank_annotations/steroid_obi_30d_21-07-06.parquet')\n",
    "steroids_30d = steroids_30d[['PatientID', 'Glucocorticoids']]\n",
    "steroids_30d['glucocorticoid'] = steroids_30d.Glucocorticoids == 1.0\n",
    "covariates_df = covariates_df.merge(steroids_30d[['PatientID', 'glucocorticoid']], how='left')\n",
    "\n",
    "transplants = \\\n",
    "pd.read_parquet('/mnt/obi0/mhomilius/projects/sysmex/data/obi_biobank_annotations/transplant_with_lvad_21-07-06.parquet')\n",
    "covariates_df = covariates_df.merge(transplants, how='left')\n",
    "\n",
    "# use this subset of covariates\n",
    "covariates_df = \\\n",
    "covariates_df[['PatientID', 'rc_consent_age', 'SexDSC', 'Race',\n",
    "               'BMI', 'TobaccoUserDSC', 'glucocorticoid', 'transplant']].\\\n",
    "rename(columns={\n",
    "    'rc_consent_age': 'Age',\n",
    "    'SexDSC': 'Sex',\n",
    "    'BMI': 'BMI',\n",
    "    'TobaccoUserDSC': 'Tobacco',\n",
    "    'glucocorticoid': 'Glucocorticoid',\n",
    "    'transplant': 'Transplant'\n",
    "})\n",
    "\n",
    "# this is based on crawling the fcs files on the server\n",
    "sysmex_fcs_meta_all = pd.read_parquet('/mnt/obi0/mhomilius/projects/sysmex/data/binned_sysmex/obi_20-09-01/meta_matched.parquet')\n",
    "sysmex_fcs_meta_all = sysmex_fcs_meta_all.drop_duplicates(subset=\"FIL\")\n",
    "\n",
    "sysmex_fcs_meta = sysmex_fcs_meta_all.\\\n",
    "reset_index().\\\n",
    "rename(columns={'index': 'sample_id'})[['sample_id', 'FIL', 'DATE', 'ETIM']]\n",
    "# 37 samples were run more than once, but de-duplicating those after merge based on FIL\n",
    "(sysmex_fcs_meta.value_counts('sample_id') > 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic input data cleanup (sample ids, perturbation ids, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for channel in ['wdf', 'ret', 'wnr', 'plt-f']:\n",
    "channel = 'wdf'\n",
    "\n",
    "print('creating directories and filtering samples')\n",
    "\n",
    "Path(f'{analysis_path}/gwas/{channel}/perturbations/').mkdir(parents=True, exist_ok=True)\n",
    "gated_stats = f'{analysis_path}/{channel.upper()}_complete.csv'\n",
    "sysmex_results = pd.read_csv(gated_stats)\n",
    "\n",
    "gated_stats2 = f'{analysis_path}/{channel.upper()}_short.csv'\n",
    "sysmex_results2 = pd.read_csv(gated_stats2).rename(columns={'Sample': 'FIL'})\n",
    "sysmex_results2 = sysmex_results2[['FIL'] + [c for c in sysmex_results2.columns if c.endswith('Percentage')]]\n",
    "\n",
    "# sysmex results contains gated stats after processing the FCS files with FlowJo\n",
    "# but otherwise only minimal filtering\n",
    "sysmex_results = sysmex_results.rename(columns={'Unnamed: 0': 'FIL'}).drop('Unnamed: 111', axis=1)\n",
    "\n",
    "# also drop the mean and sd from FlowJo\n",
    "sysmex_results = sysmex_results.loc[~sysmex_results.FIL.isin(['Mean', 'SD'])]\n",
    "\n",
    "# there should not be any FIL duplicates, but de-duplicating just in case\n",
    "sysmex_results = sysmex_results.drop_duplicates(subset='FIL')\n",
    "sysmex_results = sysmex_results.merge(sysmex_results2, left_on='FIL', right_on='FIL', how='left')\n",
    "sysmex_results = sysmex_results.merge(sysmex_fcs_meta, left_on='FIL', right_on='FIL', how='left')\n",
    "sysmex_results = sysmex_results.loc[~sysmex_results.sample_id.isna()]\n",
    "sysmex_results = sysmex_results.drop_duplicates('sample_id')\n",
    "sysmex_results['instrument'] = sysmex_results['FIL'].apply(lambda x:x.split(\"^\")[0][1:])\n",
    "\n",
    "# Indicate error files\n",
    "sysmex_results['non_error'] = sysmex_results['sample_id'].str.contains(\".*-.*-.*\",regex= True)\n",
    "sysmex_results['patient_sample_id'] = sysmex_results['sample_id'].apply(lambda x:x.rsplit(\"-\",1)[0] if any(pd.Series(x).str.contains(\".*-.*-.*\",regex= True)) else \"error\")\n",
    "sysmex_results['perturbation_num'] = sysmex_results['sample_id'].apply(lambda x:x.rsplit(\"-\",1)[1] if any(pd.Series(x).str.contains(\".*-.*-.*\",regex= True)) else \"error\")\n",
    "\n",
    "# this is the distribution of perturbations/error files\n",
    "print(sysmex_results.shape)\n",
    "print(sysmex_results['perturbation_num'].value_counts())\n",
    "\n",
    "# Remove any XN-450 samples\n",
    "sysmex_results = sysmex_results.loc[~(sysmex_results.instrument == 'XN-450')]\n",
    "\n",
    "# Remove samples from other studies\n",
    "sysmex_results = sysmex_results[~sysmex_results[\"patient_sample_id\"].str.contains(\"PET\")]\n",
    "sysmex_results = sysmex_results[~sysmex_results[\"patient_sample_id\"].str.contains(\"FH\")]\n",
    "sysmex_results = sysmex_results[~sysmex_results[\"patient_sample_id\"].str.contains(\"AZ\")]\n",
    "\n",
    "# drop any perturbation that is not in [0-36], except 20 (water on the 450)\n",
    "sysmex_results = sysmex_results.loc[sysmex_results.perturbation_num.isin(\n",
    "    [f'{i:03d}' for i in range(40) if i != 20]\n",
    ")]\n",
    "print(sysmex_results.shape)\n",
    "print(sysmex_results['perturbation_num'].value_counts())\n",
    "\n",
    "# standardize the ids so they match\n",
    "sysmex_results['standard_study_id'] = sysmex_results['patient_sample_id'].apply(standardize_study_id)\n",
    "sysmex_results = sysmex_results.merge(patient_ids[['standard_study_id', 'PatientID']],\n",
    "                     left_on='standard_study_id',\n",
    "                     right_on='standard_study_id',\n",
    "                     how='left')\n",
    "\n",
    "# removing subjects not in redcap\n",
    "print(f'subjects without patientid: {sysmex_results.loc[sysmex_results.PatientID.isna()].shape}')\n",
    "sysmex_results = sysmex_results.loc[~sysmex_results.PatientID.isna()]\n",
    "\n",
    "# simplify feature names and shorten phenotype names\n",
    "sysmex_results.columns = [re.sub('[^0-9a-zA-Z]+', '_', c) for c in sysmex_results.columns]\n",
    "sysmex_results.columns = sysmex_results.columns.str.\\\n",
    "    replace('Signal_', '').str.\\\n",
    "    replace('Side_Fluorescence_', 'SFL').str.\\\n",
    "    replace('Side_Scatter_', 'SSC').str.\\\n",
    "    replace('Forward_Scatter_', 'FSC').str.\\\n",
    "    replace('Robust_CV', 'CV').str.\\\n",
    "    replace('Robust_SD', 'SD').str.\\\n",
    "    replace('Median_', 'Med_')\n",
    "\n",
    "# add the NE2/NE4 ratio trait\n",
    "sysmex_results['NE2_NE4_ratio'] = sysmex_results.NE2_Count / sysmex_results.NE4_Count\n",
    "\n",
    "pheno_cols = list(set(sysmex_results.columns).difference(\n",
    "    [\n",
    "        'FIL', 'DATE', 'ETIM',\n",
    "        'sample_id', 'patient_sample_id', 'standard_study_id',\n",
    "        'PatientID', 'instrument', 'non_error',\n",
    "        'perturbation', 'perturbation_num'\n",
    "    ]\n",
    "))\n",
    "\n",
    "# merge with covariates and sysmex metadata like date and time of measurement\n",
    "sysmex_results = sysmex_results.merge(covariates_df, left_on='PatientID', right_on='PatientID')\n",
    "\n",
    "# drop measurments after May 2020\n",
    "sysmex_results['sysmex_datetime'] = pd.to_datetime(sysmex_results.DATE + ' ' + sysmex_results.ETIM, dayfirst=True)\n",
    "sysmex_results['DATE'] = pd.to_datetime(sysmex_results.DATE)\n",
    "sysmex_results = sysmex_results.loc[sysmex_results.DATE < pd.Timestamp(\"2020-05-01\")]\n",
    "\n",
    "# use distinct perturbation ids for 006, 007, 008 before 2019-08-14\n",
    "sysmex_results.loc[\n",
    "    (sysmex_results.perturbation_num == '006') &\n",
    "    (sysmex_results.sysmex_datetime < pd.to_datetime('2019-08-14')), 'perturbation_num'] = '037'\n",
    "\n",
    "sysmex_results.loc[\n",
    "    (sysmex_results.perturbation_num == '007') &\n",
    "    (sysmex_results.sysmex_datetime < pd.to_datetime('2019-08-14')), 'perturbation_num'] = '038'\n",
    "\n",
    "sysmex_results.loc[\n",
    "    (sysmex_results.perturbation_num == '008') &\n",
    "    (sysmex_results.sysmex_datetime < pd.to_datetime('2019-08-14')), 'perturbation_num'] = '039'\n",
    "\n",
    "# add perturbation names\n",
    "sysmex_results = sysmex_results.merge(\n",
    "    perturbation_names[['id_str', 'figure_name']].rename(\n",
    "        columns={'figure_name':'Perturbation', 'id_str': 'perturbation_num'}),\n",
    "    left_on='perturbation_num',\n",
    "    right_on='perturbation_num'\n",
    ")\n",
    "\n",
    "# save the cleaned data to disk (but outliers have not been removed yet)\n",
    "sysmex_results.to_parquet(f'{analysis_path}/{channel}_cleaned.parquet')\n",
    "sysmex_results.sysmex_datetime.dt.hour.hist(bins=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### outlier removal and plotting to see batch effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysmex_results = sysmex_results.merge(\n",
    "    redcap_blood_draws[['standard_study_id', 'SpecimenTakenTimeDTS', 'rc_consent_datetime' ]]\n",
    ")\n",
    "\n",
    "sysmex_results['draw_analysis_timedelta'] = sysmex_results.sysmex_datetime - sysmex_results.SpecimenTakenTimeDTS\n",
    "sysmex_results['draw_analysis_hours'] = sysmex_results.draw_analysis_timedelta / np.timedelta64(1, 'h')\n",
    "\n",
    "# these are suspicious/incorrect time stamps\n",
    "sysmex_results.loc[(sysmex_results.draw_analysis_hours < 0) |  (sysmex_results.draw_analysis_hours > 38)].\\\n",
    "to_excel(f'{analysis_path}/time_mismatch.xlsx')\n",
    "\n",
    "# drop the incorrect times\n",
    "sysmex_results = \\\n",
    "sysmex_results.\\\n",
    "loc[~((sysmex_results.draw_analysis_hours < 0) |  (sysmex_results.draw_analysis_hours > 38))]\n",
    "sysmex_results.draw_analysis_hours.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sysmex_results = []\n",
    "for perturb_ in tqdm(sorted(list(sysmex_results.Perturbation.unique()))):\n",
    "    perturb_results = sysmex_results.copy(deep=True).loc[sysmex_results.Perturbation == perturb_]\n",
    "    outliers = ~filter_outliers_mad(perturb_results, 'draw_analysis_hours', outlier_cutoff=2.5, return_stats=False)\n",
    "    print(f'dropping blood draw outliers: {outliers.sum()} / {len(outliers)}')\n",
    "    # removes about 1%-2% of samples for WDF\n",
    "    filtered_sysmex_results.append(perturb_results.loc[~outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysmex_results = pd.concat(filtered_sysmex_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make a copy of the phenotype data for imputation, outlier removal etc\n",
    "pheno_df = sysmex_results.copy(deep=True)\n",
    "\n",
    "print('KNN imputation')\n",
    "# impute missing values for outlier removal\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "transformed = imputer.fit_transform(pheno_df[pheno_cols])\n",
    "pheno_df[list(pheno_cols)] = transformed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('global outlier removal')\n",
    "if not Path(analysis_path + f'/projections/{channel}/').exists():\n",
    "    # project with PCA, ICA and UMAP, flag outliers and make plots with various covariates\n",
    "    pheno_df_projected = project_phenotypes(\n",
    "        pheno_df,\n",
    "        pheno_cols,\n",
    "        output_dir=analysis_path + f'/projections/{channel}/',\n",
    "        impute=False, load_embeddings=False,\n",
    "        make_plots=True, plot_outliers=True\n",
    "    )\n",
    "else:\n",
    "    # project with PCA, ICA and UMAP, flag outliers and make plots with various covariates\n",
    "    pheno_df_projected = project_phenotypes(\n",
    "        pheno_df,\n",
    "        pheno_cols,\n",
    "        output_dir=analysis_path + f'/projections/{channel}/',\n",
    "        impute=False, load_embeddings=True,\n",
    "        make_plots=False, plot_outliers=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to the original values to remove imputed values\n",
    "pheno_df_projected[pheno_cols] = sysmex_results[pheno_cols]\n",
    "\n",
    "print(f'dropping outliers: {pheno_df_projected.outlier.sum()}')\n",
    "# drops about 1% of samples (500 out of 54k) for WDF\n",
    "pheno_df_projected = pheno_df_projected.loc[~pheno_df_projected.outlier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save the processed data\n",
    "pheno_df_projected.drop(['draw_analysis_timedelta'], axis=1).to_parquet(f'{analysis_path}/{channel}_pheno_projected.parquet')\n",
    "    \n",
    "# use the outlier-filtered data as GWAS input\n",
    "gwas_input = pheno_df_projected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data for plink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop over perturbations and create input data for plink\n",
    "for perturb_ in tqdm(sorted(list(gwas_input.Perturbation.unique()))):\n",
    "    print(f'\\n\\n\\n#### {perturb_} ####')\n",
    "    perturb_results = gwas_input.copy(deep=True).loc[gwas_input.Perturbation == perturb_]\n",
    "    perturb_results = perturb_results.drop(\n",
    "        ['FIL', 'sample', 'patient_sample_id', 'instrument',\n",
    "         'Perturbation', 'standard_study_id',\n",
    "         'DATE', 'ETIM', 'Perturbation',\n",
    "         'analysis_date', 'analysis_time', 'outlier', 'non_error',\n",
    "         'SpecimenTakenTimeDTS', 'rc_consent_datetime', 'draw_analysis_timedelta'\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    output_folder = f'{analysis_path}/gwas/{channel}/perturbations/{perturb_}/'\n",
    "    if not Path(output_folder).exists():\n",
    "        Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # make a full plot of all data including quantile transformed data to understand distributions\n",
    "    pheno_data_all = perturb_results.copy(deep=True)\n",
    "    quantile_data = pheno_quantile_transform(pheno_data_all[pheno_cols], pheno_cols)\n",
    "    pheno_data_all = pheno_data_all.merge(quantile_data, left_index=True, right_index=True)\n",
    "    pheno_columns_all = sorted(pheno_cols + list(quantile_data.columns))\n",
    "    plot_pheno(pheno_data_all, pheno_columns_all, output_folder + 'pheno_all.pdf')\n",
    "    \n",
    "    # now remove outliers for each individual trait and condition\n",
    "    # first do quantile transformation, then drop anything that is still an outlier\n",
    "    print('trait-wise outlier removal')\n",
    "    quantile_data = pheno_quantile_transform(perturb_results[pheno_cols], pheno_cols)\n",
    "    \n",
    "    outlier_count = {}\n",
    "    for pheno in pheno_cols:\n",
    "        #outliers = ~filter_outliers_mad(perturb_results, pheno, outlier_cutoff=4, return_stats=False)\n",
    "        outliers = ~filter_outliers_mad(quantile_data, pheno + '-q', outlier_cutoff=4, return_stats=False)\n",
    "        if outliers.sum() < len(outliers) * 0.1:\n",
    "            outlier_count[pheno] = int(outliers.sum())\n",
    "            perturb_results.loc[outliers, pheno] = np.NaN\n",
    "    \n",
    "    with open(output_folder + 'outliers.json', 'w') as fp:\n",
    "        json.dump(outlier_count, fp)\n",
    "    #     pheno_data_filtered = perturb_results.copy(deep=True)\n",
    "    #     quantile_data = pheno_quantile_transform(pheno_data_filtered[pheno_cols], pheno_cols)\n",
    "    #     pheno_data_filtered = pheno_data_filtered.merge(quantile_data, left_index=True, right_index=True)\n",
    "    #     pheno_columns_filtered = sorted(pheno_cols + list(quantile_data.columns))\n",
    "    #     plot_pheno(pheno_data_filtered, pheno_columns_filtered, folder + 'pheno_filtered.pdf')\n",
    "    \n",
    "    # drop covariates that are not used in this run\n",
    "    perturb_results = perturb_results.drop(['BMI', 'Tobacco', 'Glucocorticoid', 'Transplant'], axis=1)\n",
    "\n",
    "    # only run gwas with at least 100 subjects\n",
    "    if perturb_results.shape[0] > 100:\n",
    "        name = perturb_\n",
    "        # use the median response if multiple available\n",
    "        perturb_results = perturb_results.groupby('PatientID').median().reset_index()            \n",
    "        # prepare_phenotype_data assumes the index is the patient id\n",
    "        perturb_results = perturb_results.set_index('PatientID')\n",
    "        \n",
    "        # write out plink input files\n",
    "        prepare_phenotype_data(\n",
    "            pheno_data=perturb_results,\n",
    "            gwas_path=gwas_path,\n",
    "            out_path=output_folder,\n",
    "            id_type='PatientID',\n",
    "            covariate_cols=['CHIP', 'BATCH', 'Sex', 'Age', 'Race', 'analysis_month', 'draw_analysis_hours'] + ['PC' + str(i+1) for i in range(10)],\n",
    "            bedfile='imputed_merged',\n",
    "            quantile_transform=True,\n",
    "        )\n",
    "\n",
    "        #perturb_results.to_parquet(\n",
    "        #    f'{output_folder}/pheno_projected_filtered.parquet')\n",
    "        perturb_results.to_csv(\n",
    "            f'{output_folder}/gwasinput.txt', sep='\\t'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single trait file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in tqdm(['wdf', 'ret', 'wnr', 'plt-f']):\n",
    "    pheno_df_projected = pd.read_parquet(f'{analysis_path}/{channel}_pheno_projected.parquet')\n",
    "    pheno_df_projected_filtered = pheno_df_projected.copy(deep=True)\n",
    "    pheno_cols = list(set(pheno_df_projected.columns).difference(\n",
    "        [\n",
    "            'FIL', 'DATE', 'ETIM',\n",
    "            'sample_id', 'patient_sample_id', 'standard_study_id',\n",
    "            'PatientID', 'instrument', 'non_error',\n",
    "            'perturbation', 'perturbation_num', 'sample', 'id_str', 'analysis_time', 'outlier',\n",
    "            'analysis_date', 'analysis_month', 'Perturbation',\n",
    "            'SpecimenTakenTimeDTS', 'rc_consent_datetime', 'draw_analysis_timedelta',\n",
    "            'draw_analysis_hours', 'sysmex_datetime', 'Sex', 'Tobacco'\n",
    "        ] + list(covariates_df.columns)\n",
    "    ))\n",
    "\n",
    "    # understand the data distributions better\n",
    "    for perturb_ in tqdm(sorted(list(pheno_df_projected.Perturbation.unique()))):\n",
    "        print(f'\\n\\n\\n#### {perturb_} ####')\n",
    "        perturb_results = pheno_df_projected.copy(deep=True).loc[pheno_df_projected.Perturbation == perturb_]\n",
    "\n",
    "        # now drop outliers for each individual trait and condition\n",
    "        # first do quantile transformation, then drop anything that is still an outlier\n",
    "        print('trait-wise outlier removal')\n",
    "        quantile_data = pheno_quantile_transform(perturb_results[pheno_cols], pheno_cols)\n",
    "\n",
    "        outlier_count = {}\n",
    "        for pheno in pheno_cols:\n",
    "            outliers = ~filter_outliers_mad(quantile_data, pheno + '-q', outlier_cutoff=4, return_stats=False)\n",
    "            if outliers.sum() < len(outliers) * 0.1:\n",
    "                outlier_count[pheno] = int(outliers.sum())\n",
    "                perturb_results.loc[outliers, pheno] = np.NaN\n",
    "                # also drop them in the multi-perturbation dataset of blood readouts\n",
    "                pheno_df_projected_filtered.loc[perturb_results.loc[outliers].index, pheno] = np.NaN\n",
    "\n",
    "    pheno_df_projected_filtered.to_parquet(f'{analysis_path}/{channel}_pheno_projected_filtered.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a single file of all sysmex traits\n",
    "wdf_traits = pd.read_parquet(f'{analysis_path}/wdf_pheno_projected_filtered.parquet')\n",
    "ret_traits = pd.read_parquet(f'{analysis_path}/ret_pheno_projected_filtered.parquet')\n",
    "wnr_traits = pd.read_parquet(f'{analysis_path}/wnr_pheno_projected_filtered.parquet')\n",
    "pltf_traits = pd.read_parquet(f'{analysis_path}/plt-f_pheno_projected_filtered.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns = [\n",
    "    'sample', 'patient_sample_id', 'perturbation_num', 'standard_study_id',\n",
    "    'PatientID', 'Age' , 'Sex', 'Race', 'BMI', 'Tobacco', 'Glucocorticoid', 'Transplant', 'Perturbation',\n",
    "    'SpecimenTakenTimeDTS', 'rc_consent_datetime', 'draw_analysis_hours', 'sysmex_datetime',\n",
    "    'analysis_time', 'analysis_month', 'analysis_date', 'instrument', 'ETIM', 'DATE'\n",
    "             ]\n",
    "\n",
    "wdf_traits = wdf_traits[id_columns + [c for c in wdf_traits.columns if not c in id_columns]]\n",
    "wnr_traits = wnr_traits[id_columns + [c for c in wnr_traits.columns if not c in id_columns]]\n",
    "ret_traits = ret_traits[id_columns + [c for c in ret_traits.columns if not c in id_columns]]\n",
    "pltf_traits = pltf_traits[id_columns + [c for c in pltf_traits.columns if not c in id_columns]]\n",
    "\n",
    "wdf_traits.columns = id_columns + ['wdf_' + c for c in wdf_traits.columns if not c in id_columns]\n",
    "wnr_traits.columns = id_columns + ['wnr_' + c for c in wnr_traits.columns if not c in id_columns]\n",
    "pltf_traits.columns = id_columns + ['pltf_' + c for c in pltf_traits.columns if not c in id_columns]\n",
    "ret_traits.columns = id_columns + ['ret_' + c for c in ret_traits.columns if not c in id_columns]\n",
    "\n",
    "all_traits = \\\n",
    "wdf_traits.merge(\n",
    "    wnr_traits,\n",
    "    left_on=id_columns,\n",
    "    right_on=id_columns,\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "all_traits = \\\n",
    "all_traits.merge(\n",
    "    pltf_traits,\n",
    "    left_on=id_columns,\n",
    "    right_on=id_columns,\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "all_traits = \\\n",
    "all_traits.merge(\n",
    "    ret_traits,\n",
    "    left_on=id_columns,\n",
    "    right_on=id_columns,\n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_traits.to_parquet(f'{analysis_path}/all_pheno_projected_filtered.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also calculate the median perturbation response per subject\n",
    "all_traits_median = all_traits.groupby(['Perturbation', 'PatientID']).median().reset_index()\n",
    "all_traits_median.to_parquet(f'{analysis_path}/all_pheno_projected_filtered_median.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_traits_median.to_csv(f'{analysis_path}/all_pheno_projected_filtered_median.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_traits_median.PatientID.unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
